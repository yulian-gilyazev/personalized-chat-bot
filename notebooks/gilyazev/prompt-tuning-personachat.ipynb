{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07e0f5e",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://camo.githubusercontent.com/473dd9f992924d27457650251786464f72e54121ac6e9210add0f483ca849277/68747470733a2f2f692e696d6775722e636f6d2f3765523750616e2e706e67\" width=\"40%\">  \n",
    "</div>\n",
    "\n",
    "# Distributed Bloom for Text Generation using Prompt Tuning\n",
    "\n",
    "In this example, we show how to use [prompt tuning](https://aclanthology.org/2021.emnlp-main.243.pdf) to adapt a test 6B version of the [BLOOM](https://huggingface.co/bigscience/bloom) model for a specific downstream task. We will run this model in a decentralized fashion using [Petals](https://github.com/bigscience-workshop/petals). Petals servers will maintain the BLOOM blocks (they are kept unchanged during adaptation), and the gradient descent will learn a few prefix tokens stored on a Petals client.\n",
    "\n",
    "We will adapt the BLOOM model for the chatbot task using the [Personachat](https://huggingface.co/datasets/bavard/personachat_truecased) dataset. For a given dialogue context, the model has to provide a relevant answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8526f",
   "metadata": {},
   "source": [
    "First, we have to prepare all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ffb5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.21.3\n",
      "  Using cached transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: requests in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (6.0)\n",
      "Requirement already satisfied: filelock in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (3.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (1.23.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from transformers==4.21.3) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.3) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.21.3) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from requests->transformers==4.21.3) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from requests->transformers==4.21.3) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from requests->transformers==4.21.3) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages (from requests->transformers==4.21.3) (2.1.1)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.23.1\n",
      "    Uninstalling transformers-4.23.1:\n",
      "      Successfully uninstalled transformers-4.23.1\n",
      "Successfully installed transformers-4.21.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ab6ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 110\n",
      "CUDA SETUP: Loading binary /home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda110_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagiljazev/personalized-chat-bot/env/lib/python3.8/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "NVIDIA GeForce RTX 3060 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3060 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../../../petals\")\n",
    " \n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Import a Petals model\n",
    "from src.client.remote_model import DistributedBloomForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf07b5d",
   "metadata": {},
   "source": [
    "Let's set some hyperparameters for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04ba4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bigscience/test-bloomd-6b3\" # select model you like\n",
    "INITIAL_PEERS = [\"/ip4/193.106.95.184/tcp/31000/p2p/QmSg7izCDtowVTACbUmWvEiQZNY4wgCQ9T9Doo66K59X6q\"] # add your peers adresses here, like \"/ip4/192.168.1.2/tcp/31000/p2p/Qma....\"\n",
    "NUM_PREFIX_TOKENS = 16\n",
    "DEVICE = 'cpu'\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-2\n",
    "WEIGHT_DECAY = 0.0\n",
    "NUM_SAMPLES = 1000\n",
    "SEED = 42\n",
    "MODEL_MAX_LENGTH = 256\n",
    "TUNING_MODE = 'ptune' # choose between ['ptune', 'deep_ptune'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38316bd",
   "metadata": {},
   "source": [
    "Prepare tokenizer and distributed model, connect it to servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c6e53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a9d9d98d164ba2af3c601484c23d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/267 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7811a101e3240608ffde2e20213bac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/13.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d482a04bdb43b6a62c304f6463cf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e42f7ad1d47c796f9441adf8f7b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3374e00deddc427f91bd3bf975db3e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Oct 22 14:15:13.116 [WARN] [/home/jagiljazev/personalized-chat-bot/notebooks/gilyazev/../../../petals/src/client/remote_sequential.py.__init__:34] RemoteSequential is in active development; expect adventures\n",
      "Some weights of DistributedBloomForCausalLM were not initialized from the model checkpoint at bigscience/test-bloomd-6b3 and are newly initialized: ['lm_head.word_embeddings.weight', 'prompt_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.model_max_length = MODEL_MAX_LENGTH\n",
    "model = DistributedBloomForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    initial_peers=INITIAL_PEERS, \n",
    "    pre_seq_len=NUM_PREFIX_TOKENS, \n",
    "    tuning_mode=TUNING_MODE\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e3786",
   "metadata": {},
   "source": [
    "Let's prepare the Personachat dataset. We need two mapping functions, one to concatenate history and candidate answers, and another for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c44d516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0a9cf6602f434898dad1c66ad4abdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1832e785a2594275b10503a57fb20458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/5.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f68b5339a40472baf5a7208f6d28b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Oct 22 14:15:24.313 [WARN] [datasets.builder._create_builder_config:427] No config specified, defaulting to: personachat_truecased/full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset personachat_truecased/full (download: 195.70 MiB, generated: 210.99 MiB, post-processed: Unknown size, total: 406.69 MiB) to /home/jagiljazev/.cache/huggingface/datasets/bavard___personachat_truecased/full/1.0.0/73ee8f1a0d9e42255af5a8301877a2f3ac638e55b1cd9cbccca5ab7e23d2b638...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bd5392dbae46d8b9b919b90fc75ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1018c7b958445a5a68194369390dd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/193M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8d543fc2fd4d5f9e42a0426cf8625b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/131438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset personachat_truecased downloaded and prepared to /home/jagiljazev/.cache/huggingface/datasets/bavard___personachat_truecased/full/1.0.0/73ee8f1a0d9e42255af5a8301877a2f3ac638e55b1cd9cbccca5ab7e23d2b638. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0f48046eec4e4cad2fc97a613c4d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a74581c7fb4fde982e7de4c1836f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb4e8c3627443f58f680a37c81ca3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea352c53e8734f0dae684aa745e59126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caa02dbd6da4244a0bb46b9b8f187f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"bavard/personachat_truecased\")\n",
    "\n",
    "\n",
    "def chunking(examples):\n",
    "    inputs = [\n",
    "        \"\\n-----\\n\".join(history) + \"\\n-----\\n\" + candidate\n",
    "        for history, candidates in zip(examples[\"history\"], examples[\"candidates\"])\n",
    "        for candidate in candidates\n",
    "    ]\n",
    "    return {\"chunks\": inputs}\n",
    "\n",
    "\n",
    "def tokenize(examples):\n",
    "    outputs = {\n",
    "        \"input_ids\": tokenizer(examples[\"chunks\"], padding='max_length', truncation=True)[\"input_ids\"]\n",
    "    }\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "tokenized_datasets = (\n",
    "    dataset\n",
    "        .map(chunking, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "        .map(tokenize, batched=True, remove_columns=[\"chunks\"])\n",
    ")\n",
    "\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=SEED)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset.select(list(range(NUM_SAMPLES))),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4323fd",
   "metadata": {},
   "source": [
    "Before setting up optimizers, check the model parameters that will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc0ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.prompt_embeddings.weight True cpu\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(n, p.requires_grad, p.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cffce7",
   "metadata": {},
   "source": [
    "The optimizer will only work on **prompts**, they are only trainable parameters. Let's initialize optimizer and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9bf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c56d5",
   "metadata": {},
   "source": [
    "Let's initialize wandb for logging and start the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e46807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                       | 1/250 [00:34<2:21:10, 34.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.633817195892334\n"
     ]
    }
   ],
   "source": [
    "# wandb.init(\n",
    "#     project=\"bloom-personachat\",\n",
    "#     config={\n",
    "#         \"num_samples\": NUM_SAMPLES,\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"learning_rate\": LR,\n",
    "#         \"weight_decay\": WEIGHT_DECAY,\n",
    "#         \"num_prefix_tokens\": NUM_PREFIX_TOKENS,\n",
    "#         \"model_name\": MODEL_NAME,\n",
    "#         \"seed\": SEED,\n",
    "#     }\n",
    "# )\n",
    "loss_hist = []\n",
    "print('wandb initialized\\n')\n",
    "\n",
    "for batch in tqdm(train_dataloader):\n",
    "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "    model.train()\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Train Loss: {loss}\")\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "#     wandb.log({\"Train Loss\": loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36cb80",
   "metadata": {},
   "source": [
    "Try to talk with the trained model! Submit an empty input to stop the execution.\n",
    "\n",
    "\n",
    "__Note__: In this example, we the whole dialogue as a prefix when generating each new replica. In the future, we will support a faster \"interactive\" dialogue mode, so generating a new replica will be able to reuse inference caches from the previous replica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720181b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 16\n",
    "TOP_K = 100\n",
    "TEMPERATURE = 0.6\n",
    "dialog = \"\"\n",
    "\n",
    "while True:\n",
    "    user_phrase = input()\n",
    "    if len(user_phrase) == 0:\n",
    "        break\n",
    "    dialog += f\"{user_phrase}\\n-----\\n\"\n",
    "    inputs = tokenizer([dialog], return_tensors='pt')['input_ids']\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        temperature=TEMPERATURE,\n",
    "        do_sample=True,\n",
    "        top_k=TOP_K,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=MAX_TOKENS,\n",
    "    )\n",
    "    bloom_answer = tokenizer.batch_decode(outputs)[0]\n",
    "    bloom_answer = bloom_answer[len(dialog):].split(\"\\n\")[0]\n",
    "    print(bloom_answer)\n",
    "    dialog += f\"{bloom_answer}\\n-----\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
